{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb, perm\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import combinations\n",
    "from igraph import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "condition = '15m'\n",
    "gap = 15*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(fr'/Volumes/SP PHD U3/{year}_PTT_raw_data/covid-19')\n",
    "output_path = Path(fr'/Volumes/SP PHD U3/{year}_PTT_raw_data/covid-19_{condition}')\n",
    "root_path_csv = Path(fr'/Volumes/SP PHD U3/{year}_PTT_raw_data/covid-19_csv')\n",
    "clustering_path = Path(fr'/Volumes/SP PHD U3/{year}_covid-19_clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: gather data (15min, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_time_gap(time_a, time_c):\n",
    "    time_format = '%Y/%m/%d %H:%M:%S'\n",
    "    timeString_a = time_a\n",
    "    struct_time_a = time.strptime(timeString_a, time_format)\n",
    "    time_stamp_a = int(time.mktime(struct_time_a))\n",
    "    timeString_c = time_c\n",
    "    struct_time_c = time.strptime(timeString_c, time_format)\n",
    "    time_stamp_c = int(time.mktime(struct_time_c))\n",
    "    time_gap = time_stamp_c - time_stamp_a\n",
    "    return time_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening iii_202102_Gossiping_covid at Sun Jul 11 17:48:34 2021\n",
      "saving 0 at Sun Jul 11 17:48:35 2021\n",
      "saving 78121 at Sun Jul 11 17:48:37 2021\n",
      "opening iii_202101_Gossiping_covid at Sun Jul 11 17:48:37 2021\n",
      "saving 0 at Sun Jul 11 17:48:38 2021\n",
      "saving 100000 at Sun Jul 11 17:48:40 2021\n",
      "saving 200000 at Sun Jul 11 17:48:42 2021\n",
      "saving 231456 at Sun Jul 11 17:48:42 2021\n",
      "opening iii_202103_Gossiping_covid at Sun Jul 11 17:48:42 2021\n",
      "saving 0 at Sun Jul 11 17:48:43 2021\n",
      "saving 54006 at Sun Jul 11 17:48:44 2021\n",
      "opening iii_202104_Gossiping_covid at Sun Jul 11 17:48:44 2021\n",
      "saving 0 at Sun Jul 11 17:48:45 2021\n",
      "saving 73481 at Sun Jul 11 17:48:46 2021\n",
      "opening iii_202105_Gossiping_covid at Sun Jul 11 17:48:46 2021\n",
      "saving 0 at Sun Jul 11 17:48:56 2021\n",
      "saving 100000 at Sun Jul 11 17:48:58 2021\n",
      "saving 200000 at Sun Jul 11 17:49:00 2021\n",
      "saving 300000 at Sun Jul 11 17:49:02 2021\n",
      "saving 400000 at Sun Jul 11 17:49:04 2021\n",
      "saving 500000 at Sun Jul 11 17:49:06 2021\n",
      "saving 600000 at Sun Jul 11 17:49:08 2021\n",
      "saving 700000 at Sun Jul 11 17:49:10 2021\n",
      "saving 800000 at Sun Jul 11 17:49:12 2021\n",
      "saving 900000 at Sun Jul 11 17:49:14 2021\n",
      "saving 1000000 at Sun Jul 11 17:49:16 2021\n",
      "saving 1100000 at Sun Jul 11 17:49:18 2021\n",
      "saving 1200000 at Sun Jul 11 17:49:20 2021\n",
      "saving 1300000 at Sun Jul 11 17:49:22 2021\n",
      "saving 1400000 at Sun Jul 11 17:49:24 2021\n",
      "saving 1500000 at Sun Jul 11 17:49:26 2021\n",
      "saving 1600000 at Sun Jul 11 17:49:28 2021\n",
      "saving 1700000 at Sun Jul 11 17:49:30 2021\n",
      "saving 1730557 at Sun Jul 11 17:49:30 2021\n",
      "opening iii_202106_Gossiping_covid at Sun Jul 11 17:49:30 2021\n",
      "saving 0 at Sun Jul 11 17:49:42 2021\n",
      "saving 100000 at Sun Jul 11 17:49:43 2021\n",
      "saving 200000 at Sun Jul 11 17:49:45 2021\n",
      "saving 300000 at Sun Jul 11 17:49:47 2021\n",
      "saving 400000 at Sun Jul 11 17:49:49 2021\n",
      "saving 500000 at Sun Jul 11 17:49:51 2021\n",
      "saving 600000 at Sun Jul 11 17:49:53 2021\n",
      "saving 700000 at Sun Jul 11 17:49:55 2021\n",
      "saving 800000 at Sun Jul 11 17:49:57 2021\n",
      "saving 900000 at Sun Jul 11 17:49:59 2021\n",
      "saving 1000000 at Sun Jul 11 17:50:01 2021\n",
      "saving 1100000 at Sun Jul 11 17:50:03 2021\n",
      "saving 1200000 at Sun Jul 11 17:50:05 2021\n",
      "saving 1300000 at Sun Jul 11 17:50:07 2021\n",
      "saving 1400000 at Sun Jul 11 17:50:09 2021\n",
      "saving 1500000 at Sun Jul 11 17:50:11 2021\n",
      "saving 1600000 at Sun Jul 11 17:50:13 2021\n",
      "saving 1700000 at Sun Jul 11 17:50:15 2021\n",
      "saving 1800000 at Sun Jul 11 17:50:17 2021\n",
      "saving 1900000 at Sun Jul 11 17:50:19 2021\n",
      "saving 2000000 at Sun Jul 11 17:50:21 2021\n",
      "saving 2028968 at Sun Jul 11 17:50:21 2021\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#csv版\n",
    "data_range = 'covid'\n",
    "df_file_name = f'{year}_{data_range}_comment_gos_{condition}_pos'\n",
    "header = ['id', 'author_id', 'from_id', 'reaction', 'post_time']\n",
    "\n",
    "with open(output_path / f'{df_file_name}.csv', 'w', newline='') as written_file:\n",
    "    writeCsv = csv.writer(written_file, delimiter=',')\n",
    "    writeCsv.writerow(header) \n",
    "\n",
    "for file in root_path_csv.glob(f'*{year}*Gossiping_{data_range}*'):\n",
    "    if '._' not in file.stem:\n",
    "\n",
    "        print(f'opening {file.stem} at {time.ctime()}')\n",
    "        comment_lst = []       \n",
    "        raw_data = pd.read_csv(file)\n",
    "        \n",
    "        for rows in raw_data.itertuples():\n",
    "            time_a = rows.time\n",
    "            time_c = rows.post_time                      \n",
    "            time_gap = count_time_gap(time_a, time_c)\n",
    "            \n",
    "            #設定條件\n",
    "            if time_gap <= gap and rows.reaction=='推' and rows.time < '2021/05/20 00:00:00':\n",
    "                comment_lst.append([rows.id, rows.author_id, rows.from_id, \n",
    "                                    rows.reaction, rows.post_time]) \n",
    "\n",
    "            if rows.Index % 100000 == 0:\n",
    "                print(f'saving {rows.Index} at {time.ctime()}')\n",
    "                with open(output_path / f'{df_file_name}.csv', 'a', newline='') as written_file:\n",
    "                    writeCsv = csv.writer(written_file, delimiter=',')\n",
    "                    writeCsv.writerows(comment_lst)                       \n",
    "                comment_lst = []     \n",
    "\n",
    "        if rows.Index % 100000 != 0:\n",
    "            print(f'saving {rows.Index} at {time.ctime()}')\n",
    "            with open(output_path / f'{df_file_name}.csv', 'a', newline='') as written_file:\n",
    "                writeCsv = csv.writer(written_file, delimiter=',')\n",
    "                writeCsv.writerows(comment_lst)   \n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184898, 5)\n"
     ]
    }
   ],
   "source": [
    "comment_gos_df = pd.read_csv(output_path / f'{year}_{data_range}_comment_gos_{condition}_pos.csv')\n",
    "print(comment_gos_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {}\n",
    "\n",
    "for x in comment_gos_df.itertuples():\n",
    "    if x.from_id in id_dict:\n",
    "        id_dict[x.from_id].add(x.id)\n",
    "    else:\n",
    "        id_dict[x.from_id] = {x.id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_threshold = 0 #出沒文章數\n",
    "acc_over_url_threshold = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in id_dict:\n",
    "    if len(id_dict[x]) >= url_threshold:\n",
    "        acc_over_url_threshold.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accounts: 26029\n",
      "above url_threshold: 26029\n",
      "combination: 338741406.0\n",
      "time: 7.33939713 minutes\n"
     ]
    }
   ],
   "source": [
    "#check the amount of accounts\n",
    "print(f'all accounts: {len(set(comment_gos_df.from_id))}')\n",
    "print(f'above url_threshold: {len(acc_over_url_threshold)}')\n",
    "print(f'combination: {comb(len(acc_over_url_threshold), 2)}')\n",
    "print(f'time: {comb(len(acc_over_url_threshold), 2)*13/10000000/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: calculate Jaccard Similarity (edge weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_covid_gos_15m_pos_inter2_sim01\n"
     ]
    }
   ],
   "source": [
    "#setting thresholds\n",
    "len_url_inter_threshold = 2\n",
    "similarity_threshold = 0.1\n",
    "ss = str(similarity_threshold).replace('.', '')\n",
    "sim_file_name = f'{year}_{data_range}_gos_{condition}_pos_inter{len_url_inter_threshold}_sim{ss}'\n",
    "print(sim_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jc_sim(a, b):\n",
    "\n",
    "    url_inter = id_dict[a].intersection(id_dict[b])\n",
    "    url_union = id_dict[a].union(id_dict[b])\n",
    "    len_url_inter = len(url_inter)\n",
    "    len_url_union = len(url_union)\n",
    "\n",
    "    if len_url_inter >= len_url_inter_threshold:\n",
    "        similarity = len_url_inter / len_url_union        \n",
    "        if similarity >= similarity_threshold:\n",
    "            outcome_similarity.add((c[0], c[1], similarity, len_url_inter, len_url_union))\n",
    "                \n",
    "    return outcome_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sun Jul 11 17:50:22 2021\n",
      "10000000 Sun Jul 11 17:50:32 2021\n",
      "20000000 Sun Jul 11 17:50:42 2021\n",
      "30000000 Sun Jul 11 17:50:52 2021\n",
      "40000000 Sun Jul 11 17:51:02 2021\n",
      "50000000 Sun Jul 11 17:51:12 2021\n",
      "60000000 Sun Jul 11 17:51:22 2021\n",
      "70000000 Sun Jul 11 17:51:32 2021\n",
      "80000000 Sun Jul 11 17:51:43 2021\n",
      "90000000 Sun Jul 11 17:51:53 2021\n",
      "100000000 Sun Jul 11 17:52:03 2021\n",
      "110000000 Sun Jul 11 17:52:13 2021\n",
      "120000000 Sun Jul 11 17:52:22 2021\n",
      "130000000 Sun Jul 11 17:52:32 2021\n",
      "140000000 Sun Jul 11 17:52:42 2021\n",
      "150000000 Sun Jul 11 17:52:52 2021\n",
      "160000000 Sun Jul 11 17:53:02 2021\n",
      "170000000 Sun Jul 11 17:53:12 2021\n",
      "180000000 Sun Jul 11 17:53:22 2021\n",
      "190000000 Sun Jul 11 17:53:31 2021\n",
      "200000000 Sun Jul 11 17:53:41 2021\n",
      "210000000 Sun Jul 11 17:53:51 2021\n",
      "220000000 Sun Jul 11 17:54:01 2021\n",
      "230000000 Sun Jul 11 17:54:12 2021\n",
      "240000000 Sun Jul 11 17:54:22 2021\n",
      "250000000 Sun Jul 11 17:54:32 2021\n",
      "260000000 Sun Jul 11 17:54:41 2021\n",
      "270000000 Sun Jul 11 17:54:51 2021\n",
      "280000000 Sun Jul 11 17:55:00 2021\n",
      "290000000 Sun Jul 11 17:55:10 2021\n",
      "300000000 Sun Jul 11 17:55:19 2021\n",
      "310000000 Sun Jul 11 17:55:29 2021\n",
      "320000000 Sun Jul 11 17:55:38 2021\n",
      "330000000 Sun Jul 11 17:55:47 2021\n",
      "338741405 Sun Jul 11 17:55:55 2021\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "outcome_similarity = set()\n",
    "\n",
    "header = ['source', 'target', 'weight', 'inter', 'union']\n",
    "with open(clustering_path / f'{sim_file_name}.csv', 'w', newline='') as written_file:\n",
    "    writeCsv = csv.writer(written_file, delimiter=',')\n",
    "    writeCsv.writerow(header) \n",
    "    \n",
    "for idx, c in enumerate(combinations(acc_over_url_threshold, 2)):\n",
    "    outcome_similarity = jc_sim(c[0], c[1])\n",
    "\n",
    "    if idx % 10000000 == 0:\n",
    "        print(idx, time.ctime())\n",
    "        with open(clustering_path / f'{sim_file_name}.csv', 'a', newline='') as written_file:\n",
    "            writeCsv = csv.writer(written_file, delimiter=',')\n",
    "            writeCsv.writerows(outcome_similarity)\n",
    "\n",
    "        outcome_similarity = set()\n",
    "\n",
    "if idx % 10000000 != 0:\n",
    "    print(idx, time.ctime())\n",
    "    with open(clustering_path / f'{sim_file_name}.csv', 'a', newline='') as written_file:\n",
    "        writeCsv = csv.writer(written_file, delimiter=',')\n",
    "        writeCsv.writerows(outcome_similarity)\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8517, 5)\n"
     ]
    }
   ],
   "source": [
    "sim_df = pd.read_csv(clustering_path / f'{sim_file_name}.csv')\n",
    "print(sim_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_df[['weight', 'inter', 'union']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0.1, weight=0.3333333333333333\n",
      "a=0.05, weight=0.5\n",
      "a=0.01, weight=0.9600000000000364\n"
     ]
    }
   ],
   "source": [
    "print(f'a=0.1, weight={sim_df.weight.quantile(q=0.90)}')\n",
    "print(f'a=0.05, weight={sim_df.weight.quantile(q=0.95)}')\n",
    "print(f'a=0.01, weight={sim_df.weight.quantile(q=0.99)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: build graph and detect communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.read_csv(clustering_path / f'{sim_file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph.TupleList([x[:3] for x in sim_df.to_numpy()], edge_attrs=['weight'], directed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH UNW- 4055 8517 -- \n",
      "+ attr: name (v), weight (e)\n"
     ]
    }
   ],
   "source": [
    "summary(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 11 17:55:55 2021\n",
      "Sun Jul 11 17:55:57 2021\n"
     ]
    }
   ],
   "source": [
    "print(time.ctime())\n",
    "communities_ifm = g.community_infomap(edge_weights='weight')\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 11 17:55:57 2021\n",
      "Sun Jul 11 17:55:57 2021\n"
     ]
    }
   ],
   "source": [
    "print(time.ctime())\n",
    "gos_pos_communities=[]\n",
    "\n",
    "for gp_idx, community in enumerate(communities_ifm):\n",
    "    if 300 > len(community) > 2:\n",
    "        gos_pos_communities.append([len(community), [g.vs[x]['name'] for x in community]])\n",
    "\n",
    "gos_pos_communities_df = pd.DataFrame(gos_pos_communities, columns=['num', 'member'])\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gos_pos_communities_df = gos_pos_communities_df.sort_values(by=['num'], ascending=False).reset_index(inplace=False)\n",
    "gos_pos_communities_df = gos_pos_communities_df.drop(columns=['index'])\n",
    "for rows in gos_pos_communities_df.itertuples():\n",
    "    gos_pos_communities_df.loc[rows.Index, 'group_index'] = f'gp_{rows.Index+1}'\n",
    "gos_pos_communities_df = gos_pos_communities_df.reindex(columns=['group_index', 'num', 'member'])\n",
    "gos_pos_communities_df.to_csv(clustering_path / f'{sim_file_name}_com.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_index</th>\n",
       "      <th>num</th>\n",
       "      <th>member</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gp_1</td>\n",
       "      <td>60</td>\n",
       "      <td>[exarawdon, acer12356, blue1232, YocoVodka, hz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gp_2</td>\n",
       "      <td>55</td>\n",
       "      <td>[mathew12310, wel1103, mybapu, sin31429, yanke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gp_3</td>\n",
       "      <td>49</td>\n",
       "      <td>[jillene, a3050909, yamason, zenar, vdml, s900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gp_4</td>\n",
       "      <td>42</td>\n",
       "      <td>[legendd, xyz530, Imgoodjob, wobbuffet, Degfxl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gp_5</td>\n",
       "      <td>36</td>\n",
       "      <td>[fish31, durarara2020, dambosrx, wanhlily, sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gp_6</td>\n",
       "      <td>29</td>\n",
       "      <td>[FutuReStronG, cwuwang, ImHoluCan, hllmayday, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gp_7</td>\n",
       "      <td>29</td>\n",
       "      <td>[s87069, littlemame, bart102617, dlter, rusa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gp_8</td>\n",
       "      <td>26</td>\n",
       "      <td>[luckyhsin199, erichha, jhoo53640, bryantiswil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gp_9</td>\n",
       "      <td>26</td>\n",
       "      <td>[vincentrufus, sam60609797, barry910543, f8605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gp_10</td>\n",
       "      <td>26</td>\n",
       "      <td>[despair78214, sony1256, bwichiro, KingJamesS,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_index  num                                             member\n",
       "0        gp_1   60  [exarawdon, acer12356, blue1232, YocoVodka, hz...\n",
       "1        gp_2   55  [mathew12310, wel1103, mybapu, sin31429, yanke...\n",
       "2        gp_3   49  [jillene, a3050909, yamason, zenar, vdml, s900...\n",
       "3        gp_4   42  [legendd, xyz530, Imgoodjob, wobbuffet, Degfxl...\n",
       "4        gp_5   36  [fish31, durarara2020, dambosrx, wanhlily, sha...\n",
       "5        gp_6   29  [FutuReStronG, cwuwang, ImHoluCan, hllmayday, ...\n",
       "6        gp_7   29  [s87069, littlemame, bart102617, dlter, rusa, ...\n",
       "7        gp_8   26  [luckyhsin199, erichha, jhoo53640, bryantiswil...\n",
       "8        gp_9   26  [vincentrufus, sam60609797, barry910543, f8605...\n",
       "9       gp_10   26  [despair78214, sony1256, bwichiro, KingJamesS,..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gos_pos_communities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gos_pos_communities_gpid = []\n",
    "\n",
    "for rows in gos_pos_communities_df.itertuples():\n",
    "    for mem in rows.member:\n",
    "        gos_pos_communities_gpid.append([mem, rows.group_index])\n",
    "        \n",
    "gos_pos_communities_gpid_df = pd.DataFrame(gos_pos_communities_gpid, columns=['id', 'group_index']).sort_values(by='group_index')\n",
    "gos_pos_communities_gpid_df.to_csv(clustering_path / f'{sim_file_name}_gpid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8161, 5)\n"
     ]
    }
   ],
   "source": [
    "node_set = set(gos_pos_communities_gpid_df['id'])\n",
    "edge_df = []\n",
    "for rows in sim_df.itertuples():\n",
    "    if rows.source in node_set and rows.target in node_set:\n",
    "        edge_df.append([\n",
    "            rows.source,\n",
    "            rows.target,\n",
    "            float(rows.weight),\n",
    "            int(rows.inter),\n",
    "            int(rows.union)\n",
    "        ])\n",
    "edge_df = pd.DataFrame(edge_df, columns=['source', 'target', 'weight', 'inter', 'union'])\n",
    "print(edge_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df.to_csv(clustering_path / f'{sim_file_name}_edge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
